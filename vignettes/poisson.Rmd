---
title: "Poisson models for person-years data and expected rates"
author: "Elizabeth Atkinson, Cynthia Crowson, and Terry Therneau"
date: '`r format(Sys.time(),"%d %B, %Y")`'
bibliography: refer.bib
output: 
    bookdown::html_document2:
        base_format: rmarkdown::html_vignette
        number_sections: true
        toc: true
        code_folding: hide
        code_download: true
vignette: >
  %\VignetteIndexEntry{Poisson models for person-years data and expected rates}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(survival)
library(tidyverse)

## turn off certain options
options(stringsAsFactors=FALSE, show.signif.stars=FALSE)

## ggplot options
theme_set(theme_classic())

## packages to control output/tables
library(knitr)
library(rmarkdown)
library(kableExtra)

opts_chunk$set(comment = NA, echo=FALSE, message=FALSE, warning=FALSE, collapse=TRUE)
```

# Introduction

## Motivation

In medical research we are often faced with the question of whether, in a
specified cohort, the observed number of events (such as death or fracture) is
more than we would expect in the general population. If there is an excess risk,
we then wish to determine whether the excess varies based on factors such as
age, sex, and time since disease onset. Another question often asked is how do
two different time scales, such as calendar year and time since disease onset,
both contribute to the rate of an event or the excess risk.

Statistical approaches to this problem have been studied for a long time
and there is a well-described set of methods available (see, for
instance, overviews in the Encyclopedia of Biostatistics @Inskip98). An
interesting aspect of the methods, and the primary subject of this
report, is that many of the computations are very closely related to
Poisson regression models. Powerful modern software, such as the
generalized linear models functions of R, allow us to do 
these "specialized" computations quite simply
via creation of datasets in the appropriate format. This report
summarizes several of these computations, and is also a compendium of
various tricks and techniques that the authors have accumulated over the
years.

At the outset, however, we need to distinguish two separate kinds of
questions about event rates, as the approaches herein deal only with one
of them. Consider all of the subjects in a population with a particular
condition, e.g. exposure to Lyme disease as measured by the presence of
an antibody. Interest could center on two quite different questions:

-   Prospective risk: a set of subjects with the condition has been
    identified; what does their future hold?
-   Population risk: what is the impact of the condition on the
    population as a whole? This includes all subjects with the
    condition, whether currently identified or not.

A big difference between these questions is what to do with cases that
are found post-hoc, for instance patients who are found to have the
antibody only when they develop an adverse outcome known to be related
to Lyme disease, or cases found at autopsy. Analysis for population
questions is much more difficult since it involves not only the cases
found incidentally, but inference about the number of subjects in the
population with the condition of interest who have not been identified.
The methods discussed here only apply to the prospective case. We might
think of prospective risk as the therapist's question, as they advise
the patient currently in front of them about his/her future. Interesting
questions include

-   Short term. What is the risk for the short term, say 30 days? This
    question is important, but we must be quite careful about implying
    causality from any association that we find. We might find more
    joint pain in Lyme disease patients simply because we look for it
    harder, or a particular lab test might only be ordered on selected
    patients.

-   Long term. What is the long term risk for the patient? Specific
    examples of questions are:

    -   Do patients with diabetes have a higher death rate than the
        general population? Has this relationship changed over time? Are
        there different relationships for younger versus older subjects?
        Does this relationship change with disease duration? Has this relationship
        changed over calendar periods (such as during 2020)?
    –   Has the rheumatoid arthritis population experienced the same survival
        benefits as the general population? 
    –   Multiple myeloma subjects
        are known to experience a large number of fractures. Does this
        excess fracture rate occur before or after the diagnosis of
        multiple myeloma? Is this excess fracture rate constant after
        diagnosis? 
    –   Do patients with MGUS (monoclonal gammopathy of
        undetermined significance) have higher death rates than
        expected? Is the excess mortality rate constant, or does it rise
        with age? How is the amount of excess related to gender,
        obesity, or other factors?

Each of these items are actual medical questions that have been
addressed in studies at Mayo, worked on by the authors.  For purposes of
reproducibility, we have restricted the examples to those found in the
survival package. Although perhaps not as exciting, they do illustrate 
the approaches that can be applied to other projects.


## Person-years

Tabular analyses of observed versus expected events, often called
"person-years" summaries, are very familiar in epidemiological studies.
Table \@ref(tab:table1) is an example, and shows how a subject may contribute to
multiple cells within a person-years table. In this case, the results
have been stratified both by patient age and by the time interval that
has transpired since detection of MGUS. This table includes the
follow-up for a female who is age 64 at diagnosis of MGUS and is
followed for approximately 4.5 years. She contributes 0.082 years (1
month) to the "0-1 month" cell as a 55-65 year old. During the "6-12
month" cell she contributes person-years to two different age groups.


Table: (\#tab:table1) Person-years analysis for one person who entered the study at
age 64 and died at age 68.9. Each cell contains the number of
person-years spent in that category.

```{r}
library(survival)
tmpdata <- mgus[mgus$id==185,]

cuttime1 <- tcut(rep(0, nrow(tmpdata)), c(0, 30, 182, 365.25*c(1, 2, 5, 10, 100)),
              labels=c('0-1 mon', '1-6 mon', '6-12 mon', '1-2 yr',
                       '2-5 yr', '5-10 yr', '10+ yr'))
              
cutage1  <- tcut(tmpdata$age*365.25, c(0, 35,45,55,65,75, 110)*365.25, 
              labels=c('<35', '35-45', '45-55', '55-65', '65-75', '75+'))

tmp2 <- pyears(Surv(futime, death) ~ cuttime1 + cutage1, data=tmpdata, data.frame=T)
summary(tmp2, header=F, n=F, event=F, digits=2)
```

This concept can be applied to the entire mgus dataset, as
shown in the table below. We can learn a lot from this table, but it is not
always easy to read. We immediately see that making those under age 35
into a separate group was unnecessary; very few people are diagnosed
with MGUS before age 45. 

Table: (\#tab:table2) Rates analysis for patients with MGUS. Each cell
contains five values: 1) the number of subjects contributing to the
cell, 2) the number of deaths, 3)
the total number of person-years of observation in the cell, 
4) the expected number of deaths based on the
Minnesota population, and 5) a risk ratio. A given patient will
contribute to multiple cells during their follow-up.

```{r}
## Create desired grouping of the follow-up person-years
cuttime <- tcut(rep(0, nrow(mgus)), c(0, 182, c(1, 5, 100)*365.25),
                labels=c('0-6 mon', '6-12 mon', '1-5 yr', '5+ yr'))
## Create desired grouping of age
## Note - the first argument defines the age at which people enter the study.
## The tcut function then categorizes person-years according to the
## specified age groups throughout follow-up. The age categories
## and baseline age are in days but the labels are in years.
cutage <- tcut(mgus$age*365.25, c(0, 35,45,55,65,75, 110)*365.25,
               labels=c('<35', '35-45', '45-55', '55-65', '65-75', '75+'))

## create date that is 7/1/year to pull off the appropriate expected rates
mgus$dtdiag <- as.Date(paste(rep(7,nrow(mgus)),rep(1,nrow(mgus)),1900+mgus$dxyr,sep='/'),
                     format='%m/%d/%Y')

## Divide follow-up by age and time categories
pyrs.mgus <- pyears(Surv(futime, death) ~ cuttime + cutage,
                    rmap=list(age=age*365.25, year=dtdiag, sex=sex),
                    data=mgus, ratetable=survexp.mn, data.frame=T)

## create full summary
summary(pyrs.mgus, total=T, digits=1, nsmall=1)
```

```{r}
## Make sure variables are summarized correctly
## Person-years 'off table' 0 generally indicates a problem
print(pyrs.mgus)

## Look at the dimensions of the expected data
summary(survexp.mn)

## dataframe summary
dat <- pyrs.mgus$data
dat$obs_exp <- dat$event/dat$expected
dat <- dat[order(dat$cuttime, dat$cutage),]
print(dat[1:10,], digits=3)
```

The `tcut` command creates time-dependent categories especially for the
pyears computation. Its first argument is the starting point for each
person; for the cuttime variable (time since diagnosis of MGUS) each
person starts at 0 days. As a person is followed through time, they
dynamically move to another category of follow-up time at 182, 365, etc.
days. The `pyears` function breaks the accumulated person-years into
cells based on cutage and cuttime. The created intervals are of the form
(t1, t2], meaning that the cutpoints are included on the right end of
each interval. The `survexp.mn` rate table is a special object
containing the daily death rates for Minnesota residents by age, sex,
and calendar year; the variable names of 'age', 'sex', and 'year' can be
found by summary(survexp.mn). Other rate tables may include different
dimensions. For instance, the `survexp.usr` rate table is also divided
by race. By default, the `pyears` function assumes that your dataset,
mgus in the above example, contains both the variables found in the
model statement and the variables found in the rate table, and that the
latter are on the right scale (e.g. days versus years). If the
variable names are not in your dataset you can still do the analysis by
calling the ratetable function as part of your formula. In this
particular dataset 'year' is named 'dtdiag'. Since the Minnesota rate
table contains daily hazards, this means that age should be in days, and
that the 5 'year' argument should be a Julian date (number of days since
1/1/1960) at which the subject began their follow-up. The variable sex
should be coded as ("male", "female"). Any unique abbreviation of the
character string, ignoring case, is acceptable, as is a numeric code of
(1, 2). Correctly matching the variables of the rate table is one of the
more fussy aspects of the pyears and survexp routines, and it is easy to
make an error. Be very careful as well to use only the starting values
of your variables (follow-up time at baseline, baseline age, etc.) when
using tcut. The pyears function returns two components whose primary
purpose is data checking: 

* The summary component shows how subjects mapped into the rate table at the
start of their follow-up
* The offtable shows the amount of follow-up time that did not fall into any 
of the categories specified by the formula.

**CAUTION - COMMON MISTAKES:**

1)  When using tcut, make sure that the input value reflects the
    beginning of your time period or age period. For follow-up, you
    usually start at time 0. DO NOT use your final follow-up time in
    tcut. If you have variables that reflect the start and stop time for
    each individual, make sure the age listed is the age at the start
    time.
2)  All time and age variables MUST be in the same units (in the
    previous example, days).

## Examining population event rates

Often it is useful to first examine the event rate data before comparing it to
expected rates. Using the `jasa` data, we'll look at the observed death rates
stratified by time since enrollment and baseline age group (created using `cut`
not `tcut`). Note that in this example the ratetable option is not used so there
are no expected rates in the output.

```{r}
data1 <- subset(jasa, futime >0) #drop 1 person with no follow-up
data1$id <- 1:nrow(data1)

# etime = time since enrollment, year = year of enrollment
etime <- tcut(rep(0, nrow(data1)), floor(c(0, .25, 1, 2, 5)*365.25), 
                  label= c("0-3 m", "3-12 m", "1-2 y", "2-5 y"))
data1$year <- as.numeric(lubridate::year(data1$accept.dt))
data1$base.agegp <- cut(data1$age, breaks=c(8,50,65)) 
td.agegp <- tcut(data1$age*365.25, breaks=c(8,50,65)*365.25,
                 labels=c('(8,50]','(50,65]'))

## scale the person-years in terms of months instead of the default of years
pyrs.j <- pyears(Surv(futime, fustat) ~ base.agegp + etime, data=data1, scale=30.5,
                 data.frame=T)
pyrs.j$offtable

pyrs.j2 <- pyears(Surv(futime, fustat) ~ td.agegp + etime, data=data1, scale=30.5,
                 data.frame=T)
pyrs.j2$offtable
```

The left panel shows the rates from the output table (pyrs.j) over follow-up time
stratified by baseline age while the right panel shows the rates stratified by the current age. 

```{r}
f1 <- ggplot(pyrs.j$data, aes(etime, event/pyears, 
                              color=base.agegp, group=base.agegp)) +
      geom_point() + geom_line() + theme(legend.position="top") +
      ylim(0,.4)

f2 <- ggplot(pyrs.j2$data, aes(etime, event/pyears, 
                              color=td.agegp, group=td.agegp)) +
      geom_point() + geom_line() + theme(legend.position="top") +
      ylim(0,.4)

ggpubr::ggarrange(f1,f2)
```


## Using Poisson regression to model rates

To display the raw data, the person-years were lumped
together in follow-up groupings as a way to control the noise. Finer divisions of 
time, such as
single year of age and follow-up, work better for modeling. Poisson models are
commonly used to model incidence or person-year rates. As is pointed out
by Berry [4], incidence data do not strictly follow the assumptions of a
Poisson probability model. Using the example of the probability of death
with 150 years of follow-up - the y variable, "number of events", is in
this case deterministically 1. He then shows that Poisson based
statistical modeling is still correct, a forerunner of the modern and
more general argument for this fact which is based on counting processes
and martingales [1]. Thus, modeling of rates along with hypothesis tests
and confidence intervals can be based in generalized linear models [9].
Figure 3 shows the raw data from Figure 2 along with predicted death
rates from Poisson regression models with quadratic age terms. The
predicted death rates are obtained for a range of ages and for a dummy
person-years value of 1, and are much smoother than the raw values. The
dummy value of 1 "fools" the predict function into returning event rates
rather than the number of events (which is the y variable for the
Poisson model). This works because `E(number of events) = rate*time`.
Note that the same results could be obtained by fitting one model with
the appropriate set of interactions, but fitting 4 separate models is
often easier to plot. In this example log(pyears) is used as the offset
term, as will be described in the next section.

```{r}
##### CODE TO CREATE FIGURE 3 #####
## Finely divide age for the fit (single year intervals)
cutage4 <- tcut(data.spe$age, 365.25*c(0,seq(35,100),110), labels=c('<35', 35:99, '100+'))
pyrs.spe4 <- pyears(Surv(futime, status) ~ cutage4 + sex + mgus + cuttime3,
data=data.spe, data.frame=T)$data
pyrs.spe4 <- pyrs.spe4[pyrs.spe4$cuttime3 == '2+',] # keep follow-up of 2+ years
## assign a numeric value to each age group for plotting and modeling
pyrs.spe4$age <- (34:100)[as.numeric(pyrs.spe4$cutage4)]
## fit Poisson models
pfit4a <- glm(event ~ offset(log(pyears)) + age + age^2, data=pyrs.spe4, family=poisson)
```

Figure 3: Predicted death rates (solid line) and 95% confidence
intervals (dashed lines) for each of the four groups, along with the
observed death rates (circles) in each cell of the table.
family=poisson, subset= (sex=='female' & mgus==0))

```{r}
pfit4b <- glm(event ~ offset(log(pyears)) + age + age^2, data=pyrs.spe4,
family=poisson, subset= (sex=='male' & mgus==0))
pfit4c <- glm(event ~ offset(log(pyears)) + age + age^2, data=pyrs.spe4,
family=poisson, subset= (sex=='female' & mgus==1))
pfit4d <- glm(event ~ offset(log(pyears)) + age + age^2, data=pyrs.spe4,
family=poisson, subset= (sex=='male' & mgus==1))
# First panel of the plot
tempx <- 40:99 #desired age range for the plot
pred <- predict(pfit4a, newdata= data.frame(age=tempx, pyears=1), se.fit=T)
matplot(tempx, exp(cbind(pred$fit,
pred$fit - 1.96* pred$se,
pred$fit + 1.96* pred$se)),
type='l', lty=c(1,2,2), col=1)
## Add points to the figure from the coarser fit, as shown in Figure 1
points(tmp.age, tmp.y[,1], pch=1)
```

## Relating Cox and rate regression models

Cox regression is a familiar statistical tool often used to model event
data. The lung dataset is used to demonstrate the similarities of the
Cox and rate regression (Poisson) models. Note that glm requires that
the endpoint be coded as (0,1) whereas coxph can handle endpoints coded
either as (0,1) or (1,2).

```{r}
mylung <- lung ## creates local version of the lung dataset
mylung$event <- mylung$status - 1 ## so that it is coded as 0=censor/1=event
coxph(Surv(time, event) ~ age + ph.ecog, data=mylung)
```

Notice how closely the coefficients and standard errors for the Poisson
regression, which uses the number of events for each person as the $y$
variable, match those of the Cox model, which is focused on a censored
time value as the response. In fact, if the baseline hazard of the Cox
model $\lambda_0(t)$ is assumed to be constant over time, the Cox model is
equivalent to Poisson regression. One non-obvious feature of the Poisson
fit is the use of an offset term. This is based on a clever "sleight of
hand", which has its roots in the fact that a Poisson likelihood is
based on the number of events $(y)$, but that we normally want to model
not the number but rather the rate of events  $(\lambda)$. Then 

$$
\begin{eqnarray*}
 E(y_i) &=& \lambda_i t_i \nonumber \\
        &=& \left( e^{X_i \beta}\right) t_i \nonumber\\
        &=& e^{X_i \beta + \log(t_i)} 
\end{eqnarray*}
$$

We see that treating the log of time as another covariate, with a known
coefficient of 1, correctly transforms from the hazard scale to the
total number of events scale. An offset in glm models is exactly this, a
covariate with a fixed coefficient of 1. The hazard rate in a Poisson
model is traditionally modeled as $\exp(X\beta)$ (i.e. the inverse link $f(\eta) = e^{\eta}$) rather than the linear form $X\beta$, for essentially the same reason that
it is modeled that way in the Cox model: it guarantees that the hazard
rate (the expected value given the linear predictors) is positive. The
exponentiated coefficients from the Cox model are hazard ratios and
those from the Poisson model are known as standardized mortality ratios
(SMR). 
A second reason for modeling $\exp(X\beta)$, at least in the Cox model
case, is that for acute diseases (such as death following hip fractures
or myocardial infarctions) the covariates often act in a multiplicative fashion, or at least approximately so, and the multipicative
model therefore provides a better fit. Largely for these two reasons:
that the underlying code works reliably and the fit is usually ac-
ceptable, the multiplicative form of both the Cox and rate regression
(Poisson) models has become the standard. Recently there has been a
growing appreciation that it is worthwhile to summarize a study not just
in terms of relative risk (hazard ratio or SMR) but in terms of absolute
risk, the absolute amount of excess hazard experienced by a subject. An
example is provided by the well-known Women's Health Initiative (WHI)
trial of combined estrogen and progestin therapy in healthy
postmenopausal women with an intact uterus. After 5 years, there was a
26% increase in the risk of invasive breast cancer (hazard ratio 1.26,
95% CI 1.0 to 1.6) among women who were in the active treatment group as
compared to placebo [11]. It has been suggested that the results of the
WHI trial should have been reported in absolute as well as relative risk
terms [10]. Thus, WHI investigators should also have emphasized that the
annual event rates in the two arms were 0.38% and 0.30%, respectively,
leading to an increased case incidence of only 8 per 10,000 patients per
year. Given other benefits of the 10 treatment, such as a reduction in
hip fracture, a patient might take a very different view of "26% excess"
and "\< 1/1000 excess". 

Consequently, this report explores the fit of
excess risk (additive) models as well as relative risk (multiplicative)
models. In many cases, excess risk models may provide information that
is comple- mentary to the relative risk models, in others they may
provide a more succinct and superior summary. Both types of models can
be fit using Poisson regression, but the data setup and fitting process
for excess risk models is somewhat more involved and certainly far less
well known.

# Relative Risk Regression (multiplicative model)

## Basic models

Relative risk regression is simply modeling the observed events,
adjusting for the appropriate expected event rates. In this case, we'll
use Poisson regression to further explore the MGUS data. In its simplest
form, this can be written as

$$
\begin{eqnarray*}
   E(y_i) &=& \left(\lambda_{\rm age, sex} e^{X_i\beta} \right) t_i \\
          &=& (\lambda_{\rm age, sex}t_i) \,e^{X_i\beta} \\
          &=&  \Lambda_{i,{\rm age, sex},t}\, e^{X_i\beta} \\
          &=&  {e_i}e^{X_i\beta} \\
          &=&  e^{X_i\beta + log(e_i)}
\end{eqnarray*}
$$

In the above formula, λage,sex is the appropriate population hazard rate
for a particular age and sex combination (that of the ith subject), and
ei is the expected number of events over the time period of observation
for the subject, or, more accurately, the cumulative hazard $\Lambda_i(t_i)$ for
the person. In reality, the baseline hazard changes over the follow-up
time for a subject, as they move from one age group to another, and
computing it directly from the rate tables is a major bookkeeping chore.
However, keeping track of these changes and computing a correct total
expected number of events for each person is precisely what is done by
the pyears and survexp functions in R. 

Per the above formulation, the
coefficients $\beta$ in this model describe the risk for each subject relative
to that for the standard population. Programming wise, the only change
from the usual Poisson regression is the use of log(expected) instead of
log(time) as the offset. The use of an offset treats the log of the
expected as another covariate, with a known coefficient of 1. For
uncomplicated data, the function `survexp` is the easiest to use. Each of these
returns the survival probability $S_i(t) = \exp[-\Lambda_i(t)]$, from which the
expected number of events $\Lambda_i$ can easily be extracted. We will base our
expected calculations on the Minnesota life tables.

```{r}
expected <- -log(survexp(futime ~ 1, data=mgus, ratetable=survexp.mn, cohort=F))
pfit <- glm(status ~ sex + offset(log(expected)), data=mgus, family=poisson)

range(expected)

## Try again, this time subsetting the data with futime>0
## Also remove the intercept to print separate estimates for males & females
pfit <- glm(status ~ -1 + sex + offset(log(expected)), data=mgus,
family=poisson, subset=(futime>0))
```

In this analysis we needed to confront an issue that is not uncommon in
these studies: two of the subjects have an expected number of events of
0. Upon further examination, these are two people who were diagnosed
with MGUS on the day of their death. Simple relative survival is not a
valid technique when such cases are included. The model is attempting to
compare the mortality experience of the enrolled subjects to that of a
hypothetical control population, matched by age and sex, drawn randomly
from the total population of Minnesota. It recognizes, correctly, that
the probability of such a control's demise at the instant of enrollment
is zero, i.e., infinitely unlikely, which leads to infinite values in
the likelihood. The problem extends beyond day 0. In this dataset there
are 16 subjects who die within 1 week of MGUS detection; for all of
these it is almost certain that MGUS was detected because the patients
were at an extreme risk of death. We must exclude those with futime=0,
but perhaps we should also exclude those with very small follow-up
times. The analysis above shows that for both males and females, the
death rate is significantly worse than that for an age-, sex- and
calendar-year matched population. Rates are 55% greater than the
Minnesota population at large (exp(0.44) = 1.55). Note that because we
have removed the intercept (using the -1 coding), we have coefficients
for both males and females. This allows us to visually compare the
coefficients and also to obtain the correct standard error term for each
gender. In the age range of this study (mean age = 71) the population
death rate for males is substantially higher than that for females; it
is interesting that the excess death rate associated with a MGUS
diagnosis is almost identical for the two groups. To explore this
further, we will look at a second dataset that allows an estimate of
detection bias, i.e., how much of this increase is actually due to MGUS,
and how much might be due to the disease condition that caused the
patient to come to Mayo. We also want to look at time trends in the
rate: is the MGUS effect greater or less for older patients, for times
near to or far from the diagnosis date, and for different calendar
years?

## Dividing follow-up time into pieces

Normally, relative risk models will include one or more variables that
vary over the time span of the patient. These include the naturally
time-dependent ones of age and calendar year (which the relevant rate
tables also include), but may include categorical time-dependent
variables such as the initiation of a particular treatment. When
creating data for a tabular display such as Table 2 one has to break
time into moderately large chunks in order to simplify the display. When
setting the data up for regression, we may still want to use broad
categories for any variable that is to be treated as discrete categories
in the model, i.e., using a factor statement. For variables that we wish
to look at continuously, the divisions should be much finer. There are
two basic ways to create this division. The first is to preprocess the
data, dividing each person into multiple (start time, end time]
observations. This approach is often used in the creation of datasets
for a Cox model. A second is to use the person-years routines to do the
division for us and this approach is shown below. As pointed out
earlier, the very early deaths in the MGUS data present us with a
chicken-and-egg problem: did the MGUS have an impact on the death rate,
or did a state of severe disease cause detection ofMGUS? Monoclonal
gammopathy is detected from the serum protein electrophoresis (SPE)
test, which is ordered by a physician for a number of reasons. It is
often an exploratory test when the root cause for a patient's condition
is unclear. We'll now use the data.spe dataset that includes all
subjects for whom SPE was ordered, both those with a positive result
(MGUS) and those with a negative test. For this discussion we will
ignore the possibility of a calendar year effect – a more complete
analysis did not find one – and use all the available data. If there is
a short term medical impact of MGUS over and above a mere selection
effect (i.e. the type of patient on whom this test is ordered is very
ill), we will be able to see it in the difference between the negative
and positive SPE results.

```{r}
### First we create the pyrs.spe dataset
cuttime <- tcut(rep(0, nrow(data.spe)), c(0:23 *30.5, 365.25*2:10, 36525),
labels=c(paste(0:23, '-', 1:24, ' mon', sep=''),
paste(2:9, '-', 3:10, ' yr', sep=''), '10+ yr'))
cutage <- tcut(data.spe$age, 365.25*c(0,40:95,110),
labels= c("<40", paste(40:94, '-', 41:95, sep=''), "95+"))
## Save the dataset for further analysis
tmpfit <- pyears(Surv(futime, status) ~ cuttime + cutage + sex
+ mgus + ratetable(age=age, sex=sex, year=dtdiag),
data=data.spe, ratetable=survexp.mn, data.frame=T)
## Double check that the pyears, ages, sex distribution, and dates all look ok.
summary(tmpfit)

## From here forward we only use the data portion
pyrs.spe <- tmpfit$data
## Create 2 new variables based on the midpoint of each of these
## categories (such as cuttime= "0-1 mon" and cutage="<40 ")
## The use of as.numeric is a handy trick - it indicates which year
## or age group (1st, 2nd, etc.) each observation is in. The square brackets
## list a dxtime of (0 + .5)/12 + .5 = 0.0417 for every line that includes
## the first cuttime category (0-1 mon).
pyrs.spe$dxtime <- c((0:23 + .5)/12, 2:10 + .5)[as.numeric(pyrs.spe$cuttime)]
pyrs.spe$age <- c(39:95 + .5)[as.numeric(pyrs.spe$cutage)]
## Look at the first 4 observations in this new dataset
pyrs.spe[1:4,]
```

As before, we use the tcut command to create time-dependent cutpoints
for the pyears routine. We've created follow-up time intervals of zero
to 1 month, 1 to 2 months, etc. for the first 2 years, then yearly up to
10 years after the SPE test. For the age variable we have used 1 year
age groupings from age 40 up to age 95. Notice one other constraint of
rate tables: since the Minnesota rate table uses units of hazard/day,
all time variables in the dataset must be in days. The default behavior
for the pyears function is to create a set of arrays, however the
data.frame=T argument produces instead a dataset useful for further
analysis. In the final data frame, the 'cuttime' and 'cutage' variables
are categorical variables which is a result of using tcut and pyears.
The last 2 lines create a numeric value for each category which will be
more useful for subsequent models and plots.



You will run into problems if you have age in years and follow-up time
in days. Additionally, these units need to match the units used in your
rate table. For example, when the summary shows that age ranged between
0 and 0.3 years, it is a good clue that you used years and the program
expected days.

We then fit generalized additive models (gam) using the gam function.
Generalized additive models are simply an extension of the generalized
linear models that are often used for Poisson regression. Gam models
allow the fitting of nonparametric functions, in this case the smoother
function s, to estimate relationships between the event rate and the
predictors (age and dxtime). Again we use log(expected) as an offset to
describe the risk for each subject relative to that for the standard
population. Four subsets are fit, broken up by male/female and
MGUS/Negative.

```{r}
fit3.1 <- gam(event ~ offset(log(expected)) + s(age) + s(dxtime),
data=pyrs.spe, family=poisson,
subset=(sex=='female' & mgus==0))
fit3.2 <- gam(event ~ offset(log(expected)) + s(age) + s(dxtime),
data=pyrs.spe, family=poisson,
subset=(sex=='male' & mgus==0))
fit3.3 <- gam(event ~ offset(log(expected)) + s(age) + s(dxtime),
data=pyrs.spe, family=poisson,
subset=(sex=='female' & mgus==1))
fit3.4 <- gam(event ~ offset(log(expected)) + s(age) + s(dxtime),
data=pyrs.spe, family=poisson,
subset=(sex=='male' & mgus==1))
```

### Graphical displays

Plots of the relative death rates are shown in Figure 4 (versus time)
and Figure 5 (versus age). The effects shown in the figures are very
interesting. The most surprising aspect of the curves is the notable
lack of a major effect of gender in the SPE negative patients (this is
tested in the next subsection). This lack of a gender effect will make
subsequent modeling much simpler and more compact. If we were not
adjusting for overall population death rates, gender would be one of the
largest effects, due to the female longevity advantage. Figure 4 shows
that there is a time-dependent selection effect (i.e. risk associated
with being selected to have SPE) that decays rapidly over the first two
years. It says in effect that anyone who has recently had an SPE ordered
has a relatively high risk of death, independent of the outcome of that
test. It perhaps reflects on the type of patient for which a physician
would order that test. Figure 5 shows a large and decreasing age effect,
for both positive and negative SPE outcomes, but with a substantially
higher death rate for MGUS patients. We need to state two caveats with
respect to the figures. First, recognize that this is a curve for
subjects with specified covariate values and it is not representative of
the entire experience of the cohort. In order to get a representative
plot of the entire cohort we'll need to do something called direct
standardization (see section 2.3). Second, we have no particular reason
to assume that the age and diagnosis time effects would be perfectly
independent in this dataset; a complete analysis is needed to look
further at interactions of the two effects.

Figure 4: The estimated selection effect for male and female patients
who are 67-68 years old (~ 67.5 years) with positive and negative SPE. To
spread out the earlier times the x-axis is on a square root scale. Note
that the y-axis is on the log scale.

Figure 5: The estimated age effect for a patient with 17-18 months (~
1.375 years) of follow-up with positive and negative SPE. The y-axis is
on the log scale.

```{r}
##### CODE TO CREATE FIGURE 4 #####
## Note: age=67.5 corresponds to the middle age group (67-68 year olds)
newdata1 <- expand.grid(age = 67.5, dxtime=seq(0,10,length=50), expected=1)
pred1 <- cbind(predict(fit3.1, newdata=newdata1, type='response'),
predict(fit3.2, newdata=newdata1, type='response'),
predict(fit3.3, newdata=newdata1, type='response'),
predict(fit3.4, newdata=newdata1, type='response'))
matplot(sqrt(newdata1$dxtime), pred1, type='l', col=1, lty=1:4, axes=F, log='y',
xlab="Time from SPE test", ylab="Relative death rate", ylim=c(1,12))
axis(1, sqrt(c(2/12, 6/12,1,2,4,6,8,10)),
c("2/12", "6/12","1","2","4","6","8","10"), crt=0, srt=0)
axis(2, c(1.5, 2.5, 5, 10), srt=90, crt=90)
box()
legend(sqrt(2), 8, c("Negative, Female", "Negative, Male","MGUS=Positive, Female",
"MGUS=Positive, Male"), lty=1:4, bty="n")
## Figure 5 uses this set of predicted values
## Note: dxtime=1.375 corresponds to the middle time interval (17-18 months)
newdata2 <- expand.grid(age = seq(40,90,length=50), dxtime=1.375, expected=1)
pred2 <- cbind(predict(fit3.1, newdata=newdata2, type='response'),
predict(fit3.2, newdata=newdata2, type='response'),
predict(fit3.3, newdata=newdata2, type='response'),
predict(fit3.4, newdata=newdata2, type='response'))
```

Plot code for Figure 5 is similar to Figure 4 above (not shown).

### Hypothesis testing

The advantage of using a nonparametric function such as the smoother
function, s, to estimate relationships between the response and the
predictors is that few assumptions are made about the relationship. The
disadvantage is that it is difficult to look for interactions between
age and a group variable such as sex or MGUS. However, it is relatively
easy to test fixed effects by removing 1 variable at a time. The call to
anova indicates that there is no significant difference between males
and females, but a highly significant MGUS effect (i.e. positivity of
the SPE test).

```{r}
### FIT OVERALL AND SUBSETTED MODELS TO CHECK FOR SEX AND MGUS SIGNIFICANCE
fit3.overall <- gam(event ~ offset(log(expected)) + s(age) + s(dxtime) + sex + mgus,
data=pyrs.spe, family=poisson)
fit3.drop1 <- gam(event ~ offset(log(expected)) + s(age) + s(dxtime) + mgus,
data=pyrs.spe, family=poisson)
fit3.drop2 <- gam(event ~ offset(log(expected)) + s(age) + s(dxtime),
data=pyrs.spe, family=poisson)
anova(fit3.overall, fit3.drop1, fit3.drop2, test='Chi')
```

| Standardization method           | Indirect                                                                                                          | Direct                                                                                                           |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| Question                         | How many events would my study population have had if their event rate was the same as the reference population?  | How many events would the reference population have had if their event rate was the same as my study population? |
| Procedure                        | Event rates in reference population are applied to the study population.                                          | Event rates in the study population are applied to the reference population                                      |
| Reference population data needed | Age/sex stratified event rates                                                                                    | Age/sex stratified population sizes                                                                              |
Table 3: Standardization Approaches 

```{r}
## Note: exp(beta) for sex = standardized mortality ratio for sex
coef(fit3.overall) 
```

It is also possible to test whether the smoother function
is different from a simple linear or quadratic fit for the term. The
example below tests for the difference between a linear age term and the
smoother. In this case there is significant evidence that the smoother
is better at explaining the age relationship. 

```{r}
fit3.lin <- gam(event ~ offset(log(expected)) + age + s(dxtime) + mgus, data=pyrs.spe,
family=poisson) 
anova(fit3.drop1, fit3.lin, test='Chi') 
```

## Direct standardization 

The observed/expected ratios shown in Table 2 are
referred to as indirect standardization or, more commonly, standardized
mortality ratios (SMR). Another statistic of interest, although less
used, is referred to as direct standardization. A good tutorial on both
of these and other suggestions along with an extensive bibliography can
be found in Inskip [6]. Whereas the indirect method asks what the event
count would be in the study population (i.e. the event rates in the
reference population are applied to the study population), if it had the
rates of the parent or reference group, the direct method asks what the
event count would be in the parent population, if it had the rates of
the sample (i.e. the event rates in the study population are applied to
the reference population). For the former we needed the age/sex
stratified rates for the reference population of interest. For the
latter we need the age/sex stratified reference population sizes (Table
3). 

Direct standardization is often used to compare the average event
rates for two or more studies, particularly when they were assessed on
different populations, e.g. white/black, or different geographic
regions, e.g. Olmsted county/Sweden. Because the underlying populations
may not have the same age/sex structure, it is not fair to directly
compare the overall study average rates from one to the other. For
instance, if one study had significantly younger enrollees, then we
would expect that the 17 overall death rate would be lower. By
normalizing them to a common population structure, the rates become
comparable. 

In direct standardization it is important to recognize the
implication of using different standard populations. For instance, if
you want to make some statement about a diseased population, you may
want to standardize to the overall age and sex distribution of that
diseased population. Often diseased populations are weighted more
heavily in the older ages, so standardizing to the US population would
give extra weight to the younger ages where there may not be as much
information. It might be more appropriate and informative to use the
overall age and sex structure of diseased subjects as a reference.
Likewise, if you have an age- and sex- stratified sampling of the
population and you want to make generalizations to the entire US
population, then you would want to standardize to the US population. The
expected number of events in the parent population is a simple sum 

$$
\begin{eqnarray*}
  D &=& R_{F,35-39}N_{F,35-39} + R_{M,35-39}N_{M,35-39} + \\
    && R_{F,40-44}N_{F,40-44} + \ldots + R_{M,95-100}N_{M,95-100}
\end{eqnarray*}
$$

where $R$ are the death rates estimated from the study
and $N$ the population sizes in the reference population. Reference rates
are usually expressed per 100,000, or 100000 $D$)/ $\sum_{i,j} N_{i,j}$, where i is
the sex and j is the age group. 

One shortcoming of direct
standardization is that covariates are limited – you can only include in
the model those variables that are known for the parent population,
which is usually age and sex groups, and sometimes race. Compare this to
the examples, where test status and time since diagnosis both played a
role. An advantage to direct standardization is that it can often be
calculated from a published paper, without access to the raw data,
allowing us to compare our work to other results. 

When doing direct
standardization, there are three advantages to using a model for the
predicted death rates rather than the table values: 

* The values for
certain age groups may be erratic due to small sample size. The
smoothing provided by the model stabilizes them. 

* We can use finer age
groupings. To see why coarse age groupings could be a problem, consider,
for example, that we had two samples to be compared, with 10 year age
groupings. In one sample the mean age within the 45-55 year age group
might be 48, and in the other 52. This could bias the comparison of the
studies. 

* Estimates of the direct age-adjusted value and its variance
can be obtained from the fitted model, as shown below. 

There is also one
major disadvantage to using a Poisson fit: the chosen model has to fit
the data well. The estimates in our example would not be particularly
good if we had used only a linear term for age, particularly in the
tails. Figure 3, which is purposely plotted on arithmetic rather than
logarithmic scale, clearly shows that the direct adjusted value depends
very heavily on the predictions in the right hand tail. 

The direct age
adjusted value and its variance can be computed as follows. Assume that
we want to standardize the rates for females with MGUS to the age 35 to
100 US population using a model that includes age and age2. From the
Poisson regression fit (using glm) we have the coefficient $\hat \beta$ and
variance-covariance matrix $\Sigma$  (i.e. `coef(pfit4c)` and
`summary(pfit4c)$cov`, respectively). If we let $X$
be the predictor matrix for the integer ages at which we need the prediction, each row of $X$ being
the vector $(1, age, age^2)$, then $\hat r =\exp(X\hat\beta)$  is the vector of predicted rates, and $T=\sum w_i \hat r_i$ is the
expected number of total events where $w_i$  is the vector of population weights, and $T/N$ will be the
direct-adjusted estimate, where $N$  is the total population. (Alternatively, use the proportions wi/N
as the weights.) The variance matrix of $X\hat\beta$ is $X \Sigma X'$, and the first-order Taylor series approximation
gives $RX\Sigma X'R$  as the variance for $\hat r$, where $R$ is a diagonal matrix with $R_{ii} = \hat r_i$.
The variance of $T$ is then $w'RX\Sigma X'R w$. 

The code below will calculate the direct age-adjusted estimate and its standard error, for the female
MGUS group. Note that this approach will not work using gam models, since in that case we do not
have an explicit variance matrix. See the appendix (section 5.4) for more details.

```{r}
## As calculated earlier in section 1.5:
pfit4c <- glm(event ~ offset(log(pyears)) + age + age^2, data=pyrs.spe4,
family=poisson, subset= (sex=='female' & mgus==1))
us.white <- sas.get('/usr/local/sasdata','us_white')
us2000 <- us.white$p2000[us.white$sex=='F' & us.white$age>=35 &
us.white$age <= 100]
USweights <- us2000*100000/sum(us2000)
ages <- 35:100
tempx <- cbind(1, ages, ages^2)
rhat <- c(exp(tempx %*% pfit4c$coef)) #predicted female rates 
rvar <- (tempx %*% summary(pfit4c)$cov.unscaled %*% t(tempx)) # variance
wrhat <- rhat * USweights #weighted rates
fest <- sum(wrhat) #rate per 100,000
fstd <- sqrt(wrhat %*% rvar %*% wrhat) #SE of the rate
cat('The direct adjusted estimate is:',round(fest), 'deaths per 100,000 +/-',round(fstd), fill=T)

## SIMILAR RESULTS USING ns() INSTEAD OF age, age^2
## create datasets subsetted to female MGUS patients
pyrs.femaleMGUS <- pyrs.spe4[pyrs.spe4$sex=='female' &
pyrs.spe4$mgus==1,]
## define knots for the ns() function
age.range <- c(35,100)
ns.age <- ns(pyrs.femaleMGUS$age, knots=c(55,65,75),
Boundary.knots=age.range) 
## fit model
agefit3.3 <- glm(event ~offset(log(pyears)) + ns.age, family=poisson, data=pyrs.femaleMGUS) 
##create age variable to include at each time point (with ns) 
PopAge <-ns(35:100, knots=c(55,65,75), Boundary.knots=age.range) 
newdata <-cbind(rep(1,nrow(PopAge)), PopAge) 
Rhat <- c(exp(newdata %%coef(agefit3.3))) 
weighted.Rhat <- matrix(Rhat*USweights,nrow=1) 
Rvar <- newdata %*% summary(agefit3.3)$cov.unscaled %*% t(newdata)
FinalEstimate <- sum(weighted.Rhat)
FinalStd <- sqrt(weighted.Rhat %*% Rvar %*% t(weighted.Rhat))
cat('The direct adjusted estimate is:',round(FinalEstimate),
'deaths per 100,000 +/-', round(FinalStd),fill=T)
```

We could get the vector piˆri directly as a prediction from the model, along with the standard error
of each element, but since predict does not return the full variance-covariance matrix, this does not
give the variance for T , the sum of the vector.

```{r}
sum(USweights*predict(pfit4c, type='response',
newdata=data.frame(age = ages, pyears = 1)))
```

One caution regarding direct standardizing to a population is that the resulting estimates often
represent a substantial extrapolation of the dataset. For instance, in the MGUS example above only
5/1384 subjects are aged < 30 years with none under 20 years. Standardization to the entire US population aged 20–100 years requires estimated rates at each age, many of which have no representatives
at all in the study subjects! Even in using the age 35–100 year subset that we chose for the examples,


Figure 6: The estimated selection effect for male and female patients with positive and negative SPE, age-
adjusted to the population of subjects who had an SPE test. To spread out the earlier times the x-axis is on a
square root scale. Note that the y-axis is on the log scale.
14% of US female population was in the 35–39 age group, and hence this age group contributed 14%
of the weight in the final estimate, but only 1.2% of the female study subjects were in this age and sex
group.

### Direct standardization to a cohort

In addition to standardizing to an external population such as the US population, it is possible to
standardize to the study population. For instance, standardizing to a cohort's baseline age distribution
can be done by averaging the curves of all the subjects in the cohort. Figure 4 shows the predicted
curve for a given age and Figure 6 shows the age-adjusted average predicted curve for the cohort of
subjects who had an SPE test ordered. As expected, the curves have the same shape as before, but
the adjusted curves have slightly different intercepts.

```{r}
##### CODE TO CREATE FIGURE 6 #####
pop.ages <- sort(data.spe$age/365.25) 
n.ages <- length(pop.ages) 
dxtime <- seq(0,10,length=50) 
newdata3 <- data.frame(expand.grid(age=pop.ages, dxtime=dxtime, expected=1)) 

##Need to do averaging for each dxtime 
tmp1 <- tapply(predict(fit3.1,newdata=newdata3, type='response'), newdata3$dxtime, mean)
tmp2 <- tapply(predict(fit3.2, newdata=newdata3, type='response'), newdata3$dxtime,
mean) 
tmp3 <- tapply(predict(fit3.3, newdata=newdata3,type='response'), newdata3$dxtime, mean)
tmp4 <- tapply(predict(fit3.4, newdata=newdata3, type='response'), newdata3$dxtime,
mean) 
pred3 <- cbind(tmp1, tmp2, tmp3, tmp4) 
```

Figure 7:
The estimated selection effect for female patients with negative SPE,
age-adjusted to the population of subjects who had an SPE test, with
confidence intervals. To spread out the earlier times the x-axis is on a
square root scale. Note that the y-axis is on the log scale.

```{r}
matplot(sqrt(dxtime), pred3, type='l', col=1, lty=1:4, axes=F, log='y',
xlab="Time from SPE test", ylab="Relative death rate", ylim=c(1,12))
axis(1, sqrt(c(2/12, 6/12,1,2,4,6,8,10)), c("2/12",
"6/12","1","2","4","6","8","10"), crt=0, srt=0) axis(2, c(1.5, 2.5,
5,10), srt=90, crt=90) box() legend(sqrt(2), 10, c("Negative, Female",
"Negative, Male", "MGUS=Positive, Female", "MGUS=Positive, Male"),
lty=1:4, bty="n") 
```

### Confidence intervals for direct standardization

to a cohort In order to calculate confidence intervals, there needs to
be an easy way to extract the variance- covariance matrix from the model
(similar to when we estimated the standard error adjusting to the US
white population above). This particular example uses the same data as
fit3.1, which was fit using a generalized additive model and the
non-parametric smoothing spline s in section 2.2. Unfortunately, neither
an explicit X matrix nor a variance matrix are available from the gam
function for an s term. Therefore, it was refit using the generalized
linear model and the parametric natural spline, ns. In this example
we've divided the age distribution of the original cohort into 5-year
groups instead of using 1-year groups (or the original data). The
percentage of subjects in each age group become the weightings of the
predicted values. 

```{r}
##### CODE TO CREATE FIGURE 7 ##### 
##### FIT THE INITIAL MODEL ##### 

## Define ranges for use in the ns function 
## Defining the range from both pyrs.spe and data.spe allows us to obtain
## predictions 

## for subjects in the original dataset and in the person-years data, which may have 
## older ages since it accounts for age at follow-up 21 
age.range <-
range(c(pyrs.spe$age, data.spe$age/365.25)) 
dx.range <-range(pyrs.spe$dxtime)
## create ns for fitting the original model
ns.age <- ns(pyrs.spe$age, knots=c(55,65, 75), Boundary.knots=age.range)
ns.dxtime <- ns(pyrs.spe$dxtime, knots=c(.25, 1,2, 5), Boundary.knots=dx.range)
glmfit3.1 <- glm(event ~ ns.age + ns.dxtime + offset(log(expected)), family=poisson,
data=pyrs.spe, subset= (sex == "female" & mgus==0))

##### PREDICTION SET-UP #####
## look at each unique dxtime in the pyrs dataset
UniqueNsDxtime <- ns(unique(pyrs.spe$dxtime),knots=c(.25,1,2,5),
Boundary.knots=dx.range) N.dxtime <- nrow(UniqueNsDxtime) 

## figure out baseline age distribution of cohort and the proportion in each age
group 

AgeWeights <- table(cut(data.spe$age/365, breaks=seq(20,105,5), left.include=T))/N
N.age <- length(AgeWeights)
## create age variable to include at each time point (with ns)
PopAge <- ns(seq(20,100,5)+2.5, knots=c(55,65,75), Boundary.knots=age.range)
## initialize storage space for final results (at each unique dxtime)
finalRhat.vector <- rep(NA, N.dxtime)
finalStd.vector <- rep(NA, N.dxtime)

##### CALCULATE FOR EACH DXTIME #####
for(i in 1:N.dxtime) {
newdata.temp <- as.matrix(data.frame(expected=rep(1,N.age), ns.age=PopAge,
ns.dxtime=UniqueNsDxtime[rep( i,N.age),]))
Rhat.temp <- c(exp(newdata.temp %*% coef(glmfit3.1)))
weightedRhat.temp <- matrix(Rhat.temp*AgeWeights,nrow=1)
Rvar.temp <- newdata.temp %*% summary(glmfit3.1)$cov.unscaled %*%
t(newdata.temp) finalRhat.vector[i] <- sum(weightedRhat.temp)
finalStd.vector[i] <- sqrt(weightedRhat.temp %*% Rvar.temp %*%
t(weightedRhat.temp)) } 

##### PLOT RESULTS #####
finalResults <- cbind(finalRhat.vector, finalRhat.vector + 1.96*finalStd.vector,
                      finalRhat.vector - 1.96*finalStd.vector)
matplot(unique(pyrs.spe$dxtime), finalResults, type='l', col=c(1,2,2))
```

